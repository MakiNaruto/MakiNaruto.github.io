---
author : MakiNaruto
title : Stanford CS224n 2020 Notes2
description : 模型是如何进行翻译的

date : 2020-02-16
tags : 
  - PaperNote
  - Assignment
  - Natural Language Processing
  - NER
  - LSTM
  - Gradient Descent
  
header_img : content_img/NLP/WestWorld.jpg

---


### 引言
[命名实体识别](https://baike.baidu.com/item/%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/6968430?fr=aladdin)（Named Entity Recognition，简称NER），又称作“专名识别”，是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。
通常包括两部分：（1）实体边界识别；（2） 确定实体类别（人名、地名、机构名或其他）。
英语中的命名实体具有比较明显的形式标志（即实体中的每个词的第一个字母要大写），所以实体边界识别相对容易，任务的重点是确定实体的类别。和英语相比，汉语命名实体识别任务更加复杂，而且相对于实体类别标注子任务，实体边界的识别更加困难。

在做推荐系统或者智能音箱等应用时，NER的作用显得尤为重要，某些词的歧义可能会导致意想不到的后果。如下图，你可能会问：两辆面包车捐献给史密斯堡的是未来的学校还是叫“未来”的学校呢？

![图 1](/content_img/NLP/CS224N-02/img1.png)

为了解决这个问题，Word-Window classification分类方法被提出，主要思想是从中心词附近的单词来进行判断，中心词是不是实体。和Skip-grim算法相似，也有一个窗口。
## Word-Window classification
在进行分类训练时，上一课训练的词向量在这里派上用场了。假设现在中心词为Paris，窗口为5，每个词的词向量维度为$\mathbb{R}^{1*4}$，将一个窗口内所有的单词进行拼接后，得到拼接矩阵$x \in \mathbb{R}^{1*20}$作为训练输入。

![图 2  对Paris进行分类](/content_img/NLP/CS224N-02/img2.png  "对Paris进行分类")

然后利用神经网络进行训练。随机初始化一个矩阵$W$，定义神经元的激活函数$ f(x)$为sigmoid。
>$$ f(x)= \frac{1}{1+e^{-x}} $$

这张图的顶点的计算结果，即为Paris在这段窗口中心的得分。

![图 3 一个简单的三层神经网络示意图](/content_img/NLP/CS224N-02/img3.png "一个简单的三层神经网络示意图")

注：$a$为神经元激活函数，z为第一层神经元计算后的输出结果。为了方便$score(x)$简写为$s$。 
### Maximum Margin Objective Function
如果令S为“真”标签窗口的得分
$S = Score $_**(Museums in Paris are amazing)**_ 
令 $S_c $为“假”标签窗口的得分
$S_c = Score $_**(Not all museums in Paris)**_  
最后，定义在所有训练窗口上的优化目标函数为：
$Minimize$ $J = max(0,1 - s + s_c)$
[为什么用此优化函数的详细讲解](https://looperxx.github.io/CS224n-2019-03-Word%20Window%20Classification,Neural%20Networks,%20and%20Matrix%20Calculus/#14-maximum-margin-objective-function)
当损失函数$J$变化值大于$\Delta$时，更新权重，否则停止更新。为了更新权重，利用神经网络的反向传播来完成。

#### 神经网络计算过程：
即当$x$矩阵输入模型后，计算过程动画展示如图所示。
![图 4  Neural Network](/content_img/NLP/CS224N-02/img4.gif  "Neural Network")

具体的计算细节可以移步这里$\rightarrow $[Neural Networks: Foundations](https://looperxx.github.io/CS224n-2019-03-Word%20Window%20Classification,Neural%20Networks,%20and%20Matrix%20Calculus/#notes-03-neural-networks-backpropagation)

### 重点：反向传播
如图三所示。各层计算公式如下
>$f(z)$为激活函数，在这里为sigmoid：
>$$
\begin{aligned} &
s = u^T h \\&
h = f(z) \\&
z = Wx + b 
\end{aligned}
$$
最后的输出为：
$$score(x)=u^T f(Wx+b)$$

将神经网络简化为图的形式表达:

![简化的神经网络图](/content_img/NLP/CS224N-02/backpropagation.png)

假如现在对权重矩阵$b$进行更新，按照反向传播的计算，
计算细节及示例$\rightarrow $[Computation Graphs and Backpropagation](https://looperxx.github.io/CS224n-2019-04-Backpropagation%20and%20Computation%20Graphs/#2-computation-graphs-and-backpropagation)

$\frac{\partial s}{\partial h} = u,  \quad 
\frac{\partial s}{\partial z} = \frac{\partial f(z)}{\partial z} \frac{\partial s}{\partial h} =  \frac{\partial h}{\partial z} \frac{\partial s}{\partial h} \because(f(z) = h)\quad 
\frac{\partial s}{\partial b} = \frac{\partial s}{\partial z}
$   经过层层迭代后，发现它其实就是一个链式求导的过程。

根据链式求导法则对$b$求偏导：
>$$
\frac{\partial s}{\partial b} =  \frac{\partial s}{\partial h} \frac{\partial h}{\partial z } \frac{\partial z}{\partial b}
$$


即对其拆分，分别求$\frac{\partial s}{\partial h}、\frac{\partial h}{\partial z} 、\frac{\partial z}{\partial b} $的导数。

>tips：在求$ \frac{\partial h} {\partial z}$时，会用到如下计算
>$$ \begin{aligned}& 
f(x) = f(x_{1} ,x_{2} , ... x_{n}) \\&
\frac{\partial f}{\partial x}  = [\frac{\partial f}{\partial x_{1}},\frac{\partial f }{\partial x_{2}},...,\frac{\partial f}{\partial x_{n}}]
\end{aligned}
$$
> $$f(x) = [f_{1}(x_{1},x_{2},...x_{n}),f_{2}(x_{1},x_{2},...x_{n}),...,f_{m}(x_{1},x_{2},...x_{n})] $$
> $$\frac{\partial f}{\partial x} 
 = \begin{bmatrix}
\frac{\partial f_{1}}{\partial x_{1} } & \cdots  & \frac{\partial f_{1}}{\partial x_{n}} \\ 
 \vdots &  \ddots  & \vdots \\ 
\frac{\partial f_{m}}{\partial x_{1}} & \cdots & \frac{\partial f_{m}}{\partial x_{n}}
\end{bmatrix} 
$$
> $$(\frac{\partial f}{\partial x})_{ij}  = \frac{\partial f_{i}}{\partial x_{j}}$$
> $h = f(z)$,$h_{i} = f(z_{i})$  

计算 $ \frac{\partial h} {\partial z}$
> $$\begin{aligned}&
\frac{\partial h}{\partial z} 
= \frac{\partial h_{i}} {\partial z_{j}} =  \frac{\partial } {\partial z_{j}}(z_{i}) \\&
=\left\{\begin{matrix}
f'(z_{i}) \quad if  \quad i = j\\0 \quad if \quad othewise
\end{matrix}\right.
\end{aligned}
$$ 
即：$$ \frac{\partial h}{\partial z }  
= \begin{pmatrix}
 f'(z_1)  &  & 0  \\  
& \ddots  & \\ 
0 &  &  f'(z_n) 
\end{pmatrix} = diag(f'(z)) $$

$\frac{\partial s}{\partial h}、\frac{\partial h}{\partial z} 、\frac{\partial z}{\partial b} $的导数的计算结果如下：
>$$\begin{aligned} &
\frac{\partial s}{\partial h} = \frac{\partial}{\partial h} (u^{T}h) = u  \\&
\frac{\partial h}{\partial z} = \frac{\partial}{\partial h} (f(z)) = diag(f'(z)) \\&
\frac{\partial z}{\partial b} = \frac{\partial}{\partial h} (Wx + b) = I
\end{aligned}
$$

##### 所以$b$的更新梯度结果为：
>$$ 
\frac{\partial s}{\partial b} 
= \frac{\partial s}{\partial h } \frac{\partial h}{\partial z} \frac{\partial z}{\partial b} 
= u·diag(f'(z)) · I 
= u \circ f'(z)
$$


同样用链式法则对$W$求偏导，
>$$\frac{\partial s}{\partial W} 
= \frac{\partial s}{\partial h }  \frac{\partial h}{\partial z} \frac{\partial z}{\partial W} $$

不难发现，链式求导的前两项是一样的，令 $\delta  = \frac{\partial s}{\partial h} \frac{\partial h}{\partial z}  =  u·diag(f'(z))$，接下来仅需计算$ \frac{\partial z}{\partial W}$就行了：

>tips:
$$ \frac{\partial a^TXb }{\partial X} = ab^T$$

##### $W$的更新梯度结果为：
>$$\begin{aligned} &
\frac{\partial s}{\partial W} 
= \frac{\partial s}{\partial h } \frac{\partial h}{\partial z} \frac{\partial z}{\partial W} \\&
= \delta \frac{\partial z}{\partial W } \\&
= \delta \frac{\partial}{\partial W} (Wx + b) \\&
= \frac{\partial}{\partial W} (\delta Wx) \\&
=  \delta^Tx^T 
\end{aligned}
$$

利用梯度下降更新方法对权重矩阵进行更新
$$\theta^{new} = \theta^{old} - \alpha \bigtriangledown_{\theta}J(\theta)$$
关于学习率，正则化，激活函数，优化器等等的细节，建议看看吴恩达机器学习，应该会有收获。
#### 若有错误或者表达不明确，欢迎大家批评指正！

[[1]Stanford / Winter 2020 CS224n](http://web.stanford.edu/class/cs224n/)<br>
[[2]CS224n-2019 学习笔记(looperxx)](https://looperxx.github.io/CS224n-2019-03-Word%20Window%20Classification,Neural%20Networks,%20and%20Matrix%20Calculus/)
